{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x7ga2OGQC220"
   },
   "source": [
    "# Natural Language Processing using Doc2Vec on National Science Foundation Awards Abstracts\n",
    "---\n",
    "### Team:  \n",
    "Jacob Noble  \n",
    "Himanshu Gamit  \n",
    "Shantanu Hadap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdOYaCrjC22-"
   },
   "source": [
    "### Imports\n",
    "\n",
    "sklearn's Standard Scaler should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from gensim) (1.16.4)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from gensim) (1.11.1)\n",
      "Requirement already satisfied: boto in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.48.0)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (1.12.27)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.20.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.27 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.15.27)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.23)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.27->boto3->smart-open>=1.8.1->gensim) (2.7.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.27->boto3->smart-open>=1.8.1->gensim) (0.14)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "12pVWFUTC22_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler # Replace with SAS version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "HiqBFGEx4kNQ",
    "outputId": "29685a3c-3835-490e-a866-b2f6d25ff668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-16 16:01:19--  https://nsfdata.s3.amazonaws.com/nsfdataset.zip\n",
      "Resolving nsfdata.s3.amazonaws.com (nsfdata.s3.amazonaws.com)... 52.216.108.51\n",
      "Connecting to nsfdata.s3.amazonaws.com (nsfdata.s3.amazonaws.com)|52.216.108.51|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 358656602 (342M) [application/zip]\n",
      "Saving to: ‘nsfdataset.zip.2’\n",
      "\n",
      "nsfdataset.zip.2    100%[===================>] 342.04M  42.7MB/s    in 8.9s    \n",
      "\n",
      "2020-04-16 16:01:28 (38.6 MB/s) - ‘nsfdataset.zip.2’ saved [358656602/358656602]\n",
      "\n",
      "Archive:  nsfdataset.zip\n",
      "caution: filename not matched:  -y\n"
     ]
    }
   ],
   "source": [
    "!wget https://nsfdata.s3.amazonaws.com/nsfdataset.zip\n",
    "!unzip nsfdataset.zip -d data -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "ELme3JwFr579",
    "outputId": "60c0c11b-9150-45a7-a261-d0176933d6b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Projects:  10000 \n",
      "Total Features:  2\n"
     ]
    }
   ],
   "source": [
    "#import saspy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "selected_cols = ['abstractText','date']\n",
    "projects = pd.read_csv(\"data/nsf_proposals.csv\", usecols = selected_cols, low_memory=False) #, nrows=30000\n",
    "print (\"Total Projects: \", projects.shape[0], \"\\nTotal Features: \", projects.shape[1])\n",
    "projects.date = pd.to_datetime(projects.date.str.replace('D', 'T'))\n",
    "projects = projects.sort_values('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPmscOBnC23B"
   },
   "source": [
    "### Load in Data using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "colab_type": "code",
    "id": "WpMB6ablC23C",
    "outputId": "d7dffdab-360b-4070-d863-772f4cd0c5e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of the abstractText: 7977\n",
      "Min length of the abstractText: 5\n",
      "Avg length of the abstractText: 1468.5334029227558\n",
      "Max words abstractText: 1169\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstractText</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A research and training center for quantitativ...</td>\n",
       "      <td>1988-08-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8891</th>\n",
       "      <td>In order to understand the impact of energetic...</td>\n",
       "      <td>1988-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7780</th>\n",
       "      <td>Studies are proposed of a single electron, iso...</td>\n",
       "      <td>1988-11-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5558</th>\n",
       "      <td>Texas A&amp;M University and the University of Tex...</td>\n",
       "      <td>1988-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6669</th>\n",
       "      <td>Major objectives of this study are to determin...</td>\n",
       "      <td>1988-11-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           abstractText       date\n",
       "2     A research and training center for quantitativ... 1988-08-08\n",
       "8891  In order to understand the impact of energetic... 1988-11-25\n",
       "7780  Studies are proposed of a single electron, iso... 1988-11-29\n",
       "5558  Texas A&M University and the University of Tex... 1988-11-30\n",
       "6669  Major objectives of this study are to determin... 1988-11-30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop values without Abstract texts\n",
    "projects = projects.dropna(how='any')\n",
    "\n",
    "print(\"Max length of the abstractText:\", projects.abstractText.str.len().max())\n",
    "print(\"Min length of the abstractText:\", projects.abstractText.str.len().min())\n",
    "print(\"Avg length of the abstractText:\", projects.abstractText.apply(lambda x: len(x) - x.count(\" \")).mean())\n",
    "\n",
    "words = projects.abstractText.str.split().apply(len)\n",
    "print(\"Max words abstractText:\", words.max())\n",
    "projects.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJJZRTyCC23F"
   },
   "source": [
    "### Setup Training Set for Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "puEINuT5C23F",
    "outputId": "ee4bd738-7544-4d88-cac6-7e5fe59d9055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of abstracts:  9580\n"
     ]
    }
   ],
   "source": [
    "X_raw = projects.abstractText.str.lower().values\n",
    "num_of_docs = len(X_raw)\n",
    "print('Number of abstracts: ', num_of_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ru1PXGUC23J"
   },
   "source": [
    "### Create generator that will tokenize the training abstracts on the fly to save on memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i1pg5829C23K"
   },
   "outputs": [],
   "source": [
    "def doc_generator(input_docs_array):\n",
    "    for i, doc in enumerate(input_docs_array):\n",
    "        tokens = gensim.utils.simple_preprocess(doc)\n",
    "        yield gensim.models.doc2vec.TaggedDocument(tokens, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t7ZvV6OTC23Q"
   },
   "outputs": [],
   "source": [
    "X = doc_generator(X_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHy9VORiC23V"
   },
   "source": [
    "### Create Initial Doc2Vec Model and Training\n",
    "\n",
    "#### Skip to next section to work load in a pretrained model.  Training could take a potentially long time.\n",
    "\n",
    "We conduct the replication to Document Embedding with Paragraph Vectors (http://arxiv.org/abs/1507.07998). In this paper, they showed only DBOW results to NSF data. So we replicate this experiments using not only DBOW but also DM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "ArrJv_dvC23W",
    "outputId": "e5ac4c70-8b29-4bc6-de7f-7dc1b252a475"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/gensim/models/doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow+w,d300,n5,w8,s0.001,t2)\n",
      "CPU times: user 4.99 s, sys: 41 ms, total: 5.03 s\n",
      "Wall time: 5.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import multiprocessing\n",
    "from pprint import pprint\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "#It is training 100 epochs in 300 dimension vector space\n",
    "models = [\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, dbow_words=1, size=300, window=8, min_count=1, iter=100, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    #Doc2Vec(dm=1, dm_mean=1, size=300, window=8, min_count=1, iter =100, workers=cores),\n",
    "]\n",
    "\n",
    "models[0].build_vocab(doc_generator(X_raw))\n",
    "print(str(models[0]))\n",
    "#models[1].reset_from(models[0])\n",
    "#print(str(models[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "c5QCW4n32P7-",
    "outputId": "42212ea2-54a7-4b7c-aa78-95ecb4b90be4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:2: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.9 s, sys: 326 ms, total: 44.2 s\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "for i, model in enumerate(models):\n",
    "    model.train(doc_generator(X_raw), total_examples=len(X_raw), epochs=model.iter)\n",
    "    model.save(str(i)+'nsf_doc2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0BS4x08Y4T-_",
    "outputId": "c6dddc6b-acd5-4dd6-ef0d-e692df17dd1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec Embedding Shape: (9580, 300)\n"
     ]
    }
   ],
   "source": [
    "print(\"Doc2Vec Embedding Shape:\",model.docvecs.vectors_docs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mr5BKOvDC23c"
   },
   "outputs": [],
   "source": [
    "# model.train(doc_generator(X_raw), total_examples=num_of_docs, epochs=1)\n",
    "# model.save('nsf_doc2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gs7Qc0KOC23h"
   },
   "source": [
    "### Load Existing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "hJbB9-JzC23k",
    "outputId": "fc10c443-1d21-48cc-cb81-cee045bae332"
   },
   "outputs": [],
   "source": [
    "existing_model = '0nsf_doc2v.model' #'nsf_d2v_100.model' # Name of Existing model to load\n",
    "model = gensim.models.Doc2Vec.load(str(existing_model)) # Model is assumed to be in the shared folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "slapVISDC23n"
   },
   "source": [
    "### Testing Doc2Vec Model\n",
    "\n",
    "1. Infer Embedding Vecor for a New Abstract\n",
    "2. Generating Most Similar Articles based on document cluster. This is using cosine similary score.\n",
    "3. Storing the most similar files to S3 bucket.\n",
    "\n",
    "\n",
    "#### 1. Infer Embedding Vecor for a New Abstract\n",
    "\n",
    "First, calculating cosine similarity of New Text using Paragraph Vector. Word Vector and Document Vector are separately stored. We have to add .docvecs after model name to extract Document Vector from Doc2Vec Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "w_o5Knh0i6Qk",
    "outputId": "d102ee6c-25f1-487f-a2b2-02f3a15bf574"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-16 16:01:58--  https://nsf-bucket.s3.us-east-2.amazonaws.com/input.txt\n",
      "Resolving nsf-bucket.s3.us-east-2.amazonaws.com (nsf-bucket.s3.us-east-2.amazonaws.com)... 52.219.84.208\n",
      "Connecting to nsf-bucket.s3.us-east-2.amazonaws.com (nsf-bucket.s3.us-east-2.amazonaws.com)|52.219.84.208|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1758 (1.7K) [text/plain]\n",
      "Saving to: ‘input.txt.2’\n",
      "\n",
      "input.txt.2         100%[===================>]   1.72K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-04-16 16:01:58 (106 MB/s) - ‘input.txt.2’ saved [1758/1758]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nsf-bucket.s3.us-east-2.amazonaws.com/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iEDLXN2-jSmj"
   },
   "outputs": [],
   "source": [
    "with open('./input.txt', 'rb') as file_stream:\n",
    "    text = file_stream.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "D4T1hmbghnFx",
    "outputId": "5e310518-7b6c-41b1-b70d-4bf99915d4c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This study utilizes publicly available data from the National Science Foundation (NSF) Web Application Programming Interface (API). In this paper, various machine learning techniques are demonstrated to explore, analyze and recommend similar proposal abstracts to aid the NSF or Awardee with the Merit Review Process. These techniques extract textual context and group it with similar context. The goal of the analysis was to utilize a Doc2Vec unsupervised learning algorithms to embed NSF funding proposal abstracts text into vector space.  Once vectorized, the abstracts were grouped together using K-means clustering. These techniques together proved to be successful at grouping similar proposals together and could be used to find similar proposals to newly submitted NSF funding proposals. \r\n",
      "To perform text analysis, SAS® University Edition is used which supports SASPy, SAS® Studio and Python JupyterLab. Gensim Doc2vec is used to generate document vectors for proposal abstracts. Afterwards, document vectors were used to cluster similar abstracts using SAS® Studio KMeans Clustering Module. For visualization, the abstract embeddings were reduced to two dimensions using Principal Component Analysis (PCA) within SAS® Studio. This was then compared to a t-Distributed Stochastic Neighbor Embedding (t-SNE) dimensionality reduction technique as part of the Scikit-learn machine learning toolkit for Python.\r\n",
      "Conclusively, NSF proposal abstract text analysis can help an awardee read and improve their proposal model by identifying similar proposal abstracts from the last 24 years. It could also help NSF evaluators identify similar existing proposals that indirectly provides insights on whether a new proposal is going to be fruitful or not.\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#text = \"\"\"This study utilizes publicly available data from the National Science Foundation (NSF) Web Application Programming Interface (API). In this paper, various machine learning techniques are demonstrated to explore, analyze and recommend similar proposal abstracts to aid the NSF or Awardee with the Merit Review Process. These techniques extract textual context and group it with similar context. The goal of the analysis was to utilize a Doc2Vec unsupervised learning algorithms to embed NSF funding proposal abstracts text into vector space.  Once vectorized, the abstracts were grouped together using K-means clustering. These techniques together proved to be successful at grouping similar proposals together and could be used to find similar proposals to newly submitted NSF funding proposals. To perform text analysis, SAS® University Edition is used which supports SASPy, SAS® Studio and Python JupyterLab. Gensim Doc2vec is used to generate document vectors for proposal abstracts. Afterwards, document vectors were used to cluster similar abstracts using SAS® Studio KMeans Clustering Module. For visualization, the abstract embeddings were reduced to two dimensions using Principal Component Analysis (PCA) within SAS® Studio. This was then compared to a t-Distributed Stochastic Neighbor Embedding (t-SNE) dimensionality reduction technique as part of the Scikit-learn machine learning toolkit for Python.Conclusively, NSF proposal abstract text analysis can help an awardee read and improve their proposal model by identifying similar proposal abstracts from the last 24 years. It could also help NSF evaluators identify similar existing proposals that indirectly provides insights on whether a new proposal is going to be fruitful or not.\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9UVA8d2kC23o"
   },
   "outputs": [],
   "source": [
    "test_vector = model.infer_vector(gensim.utils.simple_preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "woPjuZXDC23s",
    "outputId": "a03d413f-1391-4a0d-e9c4-dc4544c227cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.48014164,  0.2292498 , -0.12621427, -0.7242522 , -0.20420226,\n",
       "        0.39799866, -0.09107541, -0.3656355 , -0.6984015 ,  0.6077379 ,\n",
       "       -0.7734449 ,  0.06929234, -0.27689552,  0.02980844,  0.45976412,\n",
       "       -0.54307365, -0.4842458 ,  0.1078022 , -0.3171443 , -0.10234564,\n",
       "        0.07606005, -0.24821833,  0.04033251, -0.33498216, -0.60328996,\n",
       "       -0.6001    , -0.29221055,  0.5743363 ,  0.1012374 , -0.4831736 ,\n",
       "        0.4124102 ,  0.00392349, -0.13052528,  0.18914907, -0.3898968 ,\n",
       "       -0.21136637,  0.40480727, -0.39430317,  0.16069913, -0.15890579,\n",
       "        0.05071688,  0.11542416,  0.12898336,  0.44509062,  0.49148583,\n",
       "        0.18427867, -0.21677431, -0.43187273,  0.3376419 ,  0.32490942,\n",
       "       -0.110402  , -0.41751203,  0.42074555,  0.07995483, -0.10702007,\n",
       "        0.06365649,  0.23538487, -0.17533523, -0.7302663 , -0.14668289,\n",
       "        0.1569598 ,  0.08411324, -0.21061108, -0.0579369 , -0.07535064,\n",
       "       -0.0165423 , -0.5922711 , -0.27911848, -0.24422689, -0.73536986,\n",
       "        0.05907184, -0.16379282, -0.10475636, -0.19321069,  0.3025109 ,\n",
       "        0.07116723,  0.30447978,  0.05396686, -0.02669543, -0.39041737,\n",
       "        0.61247647,  0.21968575,  0.08983514,  0.01481443,  0.48626998,\n",
       "       -0.08358725, -0.2737797 ,  0.12484085, -0.5891664 , -0.09148793,\n",
       "        0.43323952, -0.05591497, -0.6211556 , -0.63425434, -0.31248087,\n",
       "        0.11401571, -0.02757406,  0.15059768, -0.12818062,  0.36450717,\n",
       "        0.28025794, -0.20306242, -0.00497251,  0.8736392 ,  0.04013698,\n",
       "        0.16429578,  0.83280766, -0.34617946, -0.12949651,  1.1354026 ,\n",
       "        0.5844342 , -0.27321488, -0.5508902 ,  0.33676034, -0.45110798,\n",
       "       -0.12780717,  0.23446152, -0.26864657,  0.41431826,  0.6267756 ,\n",
       "        0.3075621 , -0.00621802,  0.05572673, -0.01947306, -0.12375444,\n",
       "        0.48474374,  0.60157233,  0.63279504,  0.4271076 ,  0.7749911 ,\n",
       "       -0.8162868 ,  0.02318018,  0.12887534,  0.44662386,  0.00864993,\n",
       "       -0.05658314,  0.23548384,  0.09410908,  0.57195663, -0.3902472 ,\n",
       "        0.6545921 , -0.5440049 ,  0.5616525 ,  0.1312658 ,  0.03941474,\n",
       "       -0.30064508, -0.2621935 , -0.25348032, -0.01452299,  0.84279287,\n",
       "        0.35698235, -0.23069736, -0.2541242 ,  0.6363643 , -0.02291217,\n",
       "        0.10555257, -0.54957867, -0.31326553, -0.18362984, -0.30477452,\n",
       "        0.4455619 ,  0.24334556, -0.27640706,  0.3419575 , -0.12501796,\n",
       "       -0.16327605,  0.55896026, -0.7866469 , -0.44881776,  0.85353494,\n",
       "        0.81680423,  0.18427485, -0.20769037, -0.00453727,  0.44885528,\n",
       "        0.2385493 , -0.09564918,  0.23262176,  0.23383263, -0.10485601,\n",
       "       -0.26029295,  0.40802333,  0.01981232, -0.86069685,  0.36211815,\n",
       "       -0.26964232, -0.04563297,  0.62360686,  0.17305784,  0.33242053,\n",
       "       -0.03988974, -0.02016401,  0.4186447 ,  0.2996327 , -0.16551736,\n",
       "       -0.06392451, -0.18271002, -0.030934  ,  1.134659  ,  0.03492412,\n",
       "        0.77507675,  0.2714685 ,  0.02256788, -0.47285092, -0.09396144,\n",
       "        0.15465471,  0.6677152 ,  0.52112985,  0.08071553,  0.22109346,\n",
       "       -0.2828767 , -0.79225177, -0.3745082 ,  0.10639673,  0.34719658,\n",
       "        0.45549458, -0.54985905,  0.1460531 ,  0.68564874,  0.6852446 ,\n",
       "       -0.44668877, -0.3452647 ,  0.16303174, -0.38633922, -0.32439154,\n",
       "       -0.10296855, -0.10776263,  0.19046047, -0.22621097,  0.2800369 ,\n",
       "       -0.67348367,  0.28002313, -0.60294574, -0.41689688,  0.2915991 ,\n",
       "       -0.5951398 , -0.12136547,  0.13639455, -0.15316239,  0.04766528,\n",
       "       -0.617382  ,  0.31716353,  0.06961154, -0.26090184, -0.36365688,\n",
       "        0.4646648 , -0.4745402 , -0.570956  ,  0.07969414,  0.04269649,\n",
       "       -0.6189609 , -0.12870267,  0.01856606,  0.15304129, -0.38545594,\n",
       "        0.19581358,  0.04839187, -0.3127839 ,  0.03568073,  0.1434828 ,\n",
       "        0.20078097,  0.23453309,  0.23533428, -0.46930405, -0.4645    ,\n",
       "       -0.13789254, -0.19753048, -0.11002571,  0.54842544, -0.40716577,\n",
       "        0.39855778,  0.00216465,  0.06274315,  0.0179043 ,  0.2587706 ,\n",
       "       -0.5767881 ,  0.87136054, -0.52484477, -0.09191623, -0.08033312,\n",
       "       -0.08028096, -0.08848607,  0.1065312 , -0.5344772 , -0.5200979 ,\n",
       "       -0.0644633 , -0.1905765 , -0.59205633, -0.04186705, -0.49602687,\n",
       "        0.0979816 , -0.13162154, -0.11008294, -0.49434575, -0.98947257,\n",
       "        0.45733127, -0.29393905,  0.04039591,  0.6564341 ,  0.10574609],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ukx9sWJfjCa"
   },
   "source": [
    "#### 2. Generating Most Similar Articles based on document cluster. This is using cosine similary score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "MZ8dZDfPC23v",
    "outputId": "d336034e-4a84-4b74-f430-9dad9bd712ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6880, 0.3747926950454712), (4998, 0.3733541667461395), (8597, 0.3627268075942993)]\n"
     ]
    }
   ],
   "source": [
    "similar = model.docvecs.most_similar(\n",
    "    positive=[test_vector], \n",
    "    topn=3)\n",
    "print(similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yw_EKxCmfzZT"
   },
   "source": [
    "3. Storing the most similar files to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnnZTNlBIWTT"
   },
   "outputs": [],
   "source": [
    "# file = open('top1.txt', 'w',encoding=\"utf-8\")\n",
    "# file.write(X_raw[similar[0][0]])\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WX7zmH6oC23y"
   },
   "outputs": [],
   "source": [
    "# file = open('top2.txt', 'w',encoding=\"utf-8\")\n",
    "# file.write(X_raw[similar[1][0]])\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5FWQ2EF8C24H"
   },
   "outputs": [],
   "source": [
    "# file = open('top3.txt', 'w',encoding=\"utf-8\")\n",
    "# file.write(X_raw[similar[2][0]])\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'E605CDCBEB09B713',\n",
       "  'HostId': 'F7O1HFG+UEOxNBr0IZioDhjnGaNIFRGmysaX3xexB6jVDEYr5aK+/xa9DqXE9Hg00/hcnR9XPNI=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'F7O1HFG+UEOxNBr0IZioDhjnGaNIFRGmysaX3xexB6jVDEYr5aK+/xa9DqXE9Hg00/hcnR9XPNI=',\n",
       "   'x-amz-request-id': 'E605CDCBEB09B713',\n",
       "   'date': 'Thu, 16 Apr 2020 16:02:00 GMT',\n",
       "   'etag': '\"408662cfbd020c93f25d8b75015cda76\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"408662cfbd020c93f25d8b75015cda76\"'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "s3_conn = boto3.client(\"s3\")\n",
    "s3 = boto3.resource('s3')\n",
    "object = s3.Object('nsf-bucket', 'top1.txt')\n",
    "object.put(Body=X_raw[similar[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2G0n0VRaC24J"
   },
   "source": [
    "### Text summerizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LjVXD-4jhnGK"
   },
   "source": [
    "### Create the session\n",
    "\n",
    "The session remembers our connection parameters to SageMaker. We'll use it to perform all of our SageMaker operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "ThbzqpNFkEeS",
    "outputId": "f816d871-9c3b-4244-cc39-d0105cdc2150"
   },
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sage.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JTTJoGXrhnGM"
   },
   "source": [
    "## Create Model\n",
    "\n",
    "Now we use the Model Package to create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0VdfpjcehnGN"
   },
   "outputs": [],
   "source": [
    "# Please use the appropriate ARN obtained after subscribing to the model to define 'model_package_arn'\n",
    "model_package_arn = 'arn:aws:sagemaker:us-east-2:057799348421:model-package/marketplace-text-summarizer-11-d2490248e8de20f24ae3b72d0d74654c'\n",
    "from sagemaker import ModelPackage\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "sagemaker_session = sage.Session()\n",
    "model = ModelPackage(model_package_arn=model_package_arn,\n",
    "                    role = role,\n",
    "                    sagemaker_session = sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YPPl9wQ-hnGP"
   },
   "source": [
    "## Input File\n",
    "\n",
    "Now we pull a sample input file for testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uRE7YW7yhnGQ"
   },
   "outputs": [],
   "source": [
    "top1_txt=\"s3://nsf-bucket/top1.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7rBN7cQLhnGU"
   },
   "source": [
    "## Batch Transform Job\n",
    "\n",
    "Now let's use the model built to run a batch inference job and verify it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ul_Wsbb1hnGV",
    "outputId": "7d3504fe-061d-4179-c793-350ad57e0cb7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................\u001b[34m2020-04-16T16:05:58.054:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[33m * Serving Flask app \"serve\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n",
      " * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)\u001b[0m\n",
      "\u001b[33m169.254.255.130 - - [16/Apr/2020 16:05:58] \"#033[37mGET /ping HTTP/1.1#033[0m\" 200 -\u001b[0m\n",
      "\u001b[33m169.254.255.130 - - [16/Apr/2020 16:05:58] \"#033[33mGET /execution-parameters HTTP/1.1#033[0m\" 404 -\u001b[0m\n",
      "\u001b[35m2020-04-16T16:06:00.752:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[36m2020-04-16T16:06:00.752:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m * Serving Flask app \"serve\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n",
      " * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [16/Apr/2020 16:06:00] \"#033[37mGET /ping HTTP/1.1#033[0m\" 200 -\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [16/Apr/2020 16:06:00] \"#033[33mGET /execution-parameters HTTP/1.1#033[0m\" 404 -\u001b[0m\n",
      "\u001b[34m---input--- ccr-0098072\u001b[0m\n",
      "\u001b[34medmund clarke\u001b[0m\n",
      "\u001b[34mcmu\n",
      "\u001b[0m\n",
      "\u001b[34mabstract:\n",
      "\u001b[0m\n",
      "\u001b[34mmodel checking is an automatic verification technique for concurrent systems such as sequential circuit design and communication protocols in which temporal logic specifications are checked by an exhaustive search of the state space of the concurrent system. considerable progress has been made in the last two decades, and many major companies are now using model checking. to extend the potential of the method, this project pursues several avenues of research that will enable larger hardware systems and certain software systems to be verified.\n",
      "\u001b[0m\n",
      "\u001b[34mmodel checking and theorem proving:  theorem proving avoids the state explosion problem, but relies heavily on human guidance and tends to get unmanageable for large designs.  the project attempts to find and implement a practical methodology that will combine the benefits of theorem proving and model checking and apply it to verification of processors and security protocols.\n",
      "\u001b[0m\n",
      "\u001b[34msat-based model checking: developed recently as a complementary approach to traditional model checking based on binary decision diagrams (bdds), sat solvers tend to suffer less from the state explosion problem than bdds.  this project investigates how sat solvers and bdd techniques can be further integrated to enable verification of larger systems.\n",
      "\u001b[0m\n",
      "\u001b[34msoftware model checking:  although the major successes of model checking have been in hardware, the procedure was originally developed for software.  the first paper by clarke and emerson in 1981 proposed extracting the synchronization skeleton of a concurrent program and model checking it.  advances in model checking have generated renewed interest in this approach.  this project will explore how to achieve this goal.\u001b[0m\n",
      "\u001b[34m----in bert text to json--------- [{'src': [['ccr-0098072', 'edmund', 'clarke', 'cmu', 'abstract', ':', 'model', 'checking', 'is', 'an', 'automatic', 'verification', 'technique', 'for', 'concurrent', 'systems', 'such', 'as', 'sequential', 'circuit', 'design', 'and', 'communication', 'protocols', 'in', 'which', 'temporal', 'logic', 'specifications', 'are', 'checked', 'by', 'an', 'exhaustive', 'search', 'of', 'the', 'state', 'space', 'of', 'the', 'concurrent', 'system', '.'], ['considerable', 'progress', 'has', 'been', 'made', 'in', 'the', 'last', 'two', 'decades', ',', 'and', 'many', 'major', 'companies', 'are', 'now', 'using', 'model', 'checking', '.'], ['to', 'extend', 'the', 'potential', 'of', 'the', 'method', ',', 'this', 'project', 'pursues', 'several', 'avenues', 'of', 'research', 'that', 'will', 'enable', 'larger', 'hardware', 'systems', 'and', 'certain', 'software', 'systems', 'to', 'be', 'verified', '.'], ['model', 'checking', 'and', 'theorem', 'proving', ':', 'theorem', 'proving', 'avoids', 'the', 'state', 'explosion', 'problem', ',', 'but', 'relies', 'heavily', 'on', 'human', 'guidance', 'and', 'tends', 'to', 'get', 'unmanageable', 'for', 'large', 'designs', '.'], ['the', 'project', 'attempts', 'to', 'find', 'and', 'implement', 'a', 'practical', 'methodology', 'that', 'will', 'combine', 'the', 'benefits', 'of', 'theorem', 'proving', 'and', 'model', 'checking', 'and', 'apply', 'it', 'to', 'verification', 'of', 'processors', 'and', 'security', 'protocols', '.'], ['sat-based', 'model', 'checking', ':', 'developed', 'recently', 'as', 'a', 'complementary', 'approach', 'to', 'traditional', 'model', 'checking', 'based', 'on', 'binary', 'decision', 'diagrams', '(', 'bdds', ')', ',', 'sat', 'solvers', 'tend', 'to', 'suffer', 'less', 'from', 'the', 'state', 'explosion', 'problem', 'than', 'bdds', '.'], ['this', 'project', 'investigates', 'how', 'sat', 'solvers', 'and', 'bdd', 'techniques', 'can', 'be', 'further', 'integrated', 'to', 'enable', 'verification', 'of', 'larger', 'systems', '.'], ['software', 'model', 'checking', ':', 'although', 'the', 'major', 'successes', 'of', 'model', 'checking', 'have', 'been', 'in', 'hardware', ',', 'the', 'procedure', 'was', 'originally', 'developed', 'for', 'software', '.'], ['the', 'first', 'paper', 'by', 'clarke', 'and', 'emerson', 'in', '1981', 'proposed', 'extracting', 'the', 'synchronization', 'skeleton', 'of', 'a', 'concurrent', 'program', 'and', 'model', 'checking', 'it', '.'], ['advances', 'in', 'model', 'checking', 'have', 'generated', 'renewed', 'interest', 'in', 'this', 'approach', '.'], ['this', 'project', 'will', 'explore', 'how', 'to', 'achieve', 'this', 'goal', '.']], 'tgt': [['no', 'summary', 'found']]}]\u001b[0m\n",
      "\u001b[34m[]\u001b[0m\n",
      "\u001b[32m * Serving Flask app \"serve\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n",
      " * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [16/Apr/2020 16:06:00] \"#033[37mGET /ping HTTP/1.1#033[0m\" 200 -\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [16/Apr/2020 16:06:00] \"#033[33mGET /execution-parameters HTTP/1.1#033[0m\" 404 -\u001b[0m\n",
      "\u001b[32m---input--- ccr-0098072\u001b[0m\n",
      "\u001b[32medmund clarke\u001b[0m\n",
      "\u001b[32mcmu\n",
      "\u001b[0m\n",
      "\u001b[32mabstract:\n",
      "\u001b[0m\n",
      "\u001b[32mmodel checking is an automatic verification technique for concurrent systems such as sequential circuit design and communication protocols in which temporal logic specifications are checked by an exhaustive search of the state space of the concurrent system. considerable progress has been made in the last two decades, and many major companies are now using model checking. to extend the potential of the method, this project pursues several avenues of research that will enable larger hardware systems and certain software systems to be verified.\n",
      "\u001b[0m\n",
      "\u001b[32mmodel checking and theorem proving:  theorem proving avoids the state explosion problem, but relies heavily on human guidance and tends to get unmanageable for large designs.  the project attempts to find and implement a practical methodology that will combine the benefits of theorem proving and model checking and apply it to verification of processors and security protocols.\n",
      "\u001b[0m\n",
      "\u001b[32msat-based model checking: developed recently as a complementary approach to traditional model checking based on binary decision diagrams (bdds), sat solvers tend to suffer less from the state explosion problem than bdds.  this project investigates how sat solvers and bdd techniques can be further integrated to enable verification of larger systems.\n",
      "\u001b[0m\n",
      "\u001b[32msoftware model checking:  although the major successes of model checking have been in hardware, the procedure was originally developed for software.  the first paper by clarke and emerson in 1981 proposed extracting the synchronization skeleton of a concurrent program and model checking it.  advances in model checking have generated renewed interest in this approach.  this project will explore how to achieve this goal.\u001b[0m\n",
      "\u001b[32m----in bert text to json--------- [{'src': [['ccr-0098072', 'edmund', 'clarke', 'cmu', 'abstract', ':', 'model', 'checking', 'is', 'an', 'automatic', 'verification', 'technique', 'for', 'concurrent', 'systems', 'such', 'as', 'sequential', 'circuit', 'design', 'and', 'communication', 'protocols', 'in', 'which', 'temporal', 'logic', 'specifications', 'are', 'checked', 'by', 'an', 'exhaustive', 'search', 'of', 'the', 'state', 'space', 'of', 'the', 'concurrent', 'system', '.'], ['considerable', 'progress', 'has', 'been', 'made', 'in', 'the', 'last', 'two', 'decades', ',', 'and', 'many', 'major', 'companies', 'are', 'now', 'using', 'model', 'checking', '.'], ['to', 'extend', 'the', 'potential', 'of', 'the', 'method', ',', 'this', 'project', 'pursues', 'several', 'avenues', 'of', 'research', 'that', 'will', 'enable', 'larger', 'hardware', 'systems', 'and', 'certain', 'software', 'systems', 'to', 'be', 'verified', '.'], ['model', 'checking', 'and', 'theorem', 'proving', ':', 'theorem', 'proving', 'avoids', 'the', 'state', 'explosion', 'problem', ',', 'but', 'relies', 'heavily', 'on', 'human', 'guidance', 'and', 'tends', 'to', 'get', 'unmanageable', 'for', 'large', 'designs', '.'], ['the', 'project', 'attempts', 'to', 'find', 'and', 'implement', 'a', 'practical', 'methodology', 'that', 'will', 'combine', 'the', 'benefits', 'of', 'theorem', 'proving', 'and', 'model', 'checking', 'and', 'apply', 'it', 'to', 'verification', 'of', 'processors', 'and', 'security', 'protocols', '.'], ['sat-based', 'model', 'checking', ':', 'developed', 'recently', 'as', 'a', 'complementary', 'approach', 'to', 'traditional', 'model', 'checking', 'based', 'on', 'binary', 'decision', 'diagrams', '(', 'bdds', ')', ',', 'sat', 'solvers', 'tend', 'to', 'suffer', 'less', 'from', 'the', 'state', 'explosion', 'problem', 'than', 'bdds', '.'], ['this', 'project', 'investigates', 'how', 'sat', 'solvers', 'and', 'bdd', 'techniques', 'can', 'be', 'further', 'integrated', 'to', 'enable', 'verification', 'of', 'larger', 'systems', '.'], ['software', 'model', 'checking', ':', 'although', 'the', 'major', 'successes', 'of', 'model', 'checking', 'have', 'been', 'in', 'hardware', ',', 'the', 'procedure', 'was', 'originally', 'developed', 'for', 'software', '.'], ['the', 'first', 'paper', 'by', 'clarke', 'and', 'emerson', 'in', '1981', 'proposed', 'extracting', 'the', 'synchronization', 'skeleton', 'of', 'a', 'concurrent', 'program', 'and', 'model', 'checking', 'it', '.'], ['advances', 'in', 'model', 'checking', 'have', 'generated', 'renewed', 'interest', 'in', 'this', 'approach', '.'], ['this', 'project', 'will', 'explore', 'how', 'to', 'achieve', 'this', 'goal', '.']], 'tgt': [['no', 'summary', 'found']]}]\u001b[0m\n",
      "\u001b[32m[]\u001b[0m\n",
      "\u001b[34m[('../json_data/cnndm.valid.0.json', Namespace(dataset='', log_file='../logs/preprocess.log', lower=True, map_path='../data/', max_nsents=100, max_src_ntokens=200, min_nsents=3, min_src_ntokens=5, mode='format_to_bert', n_cpus=4, oracle_mode='greedy', raw_path='../json_data', save_path='../json_data', shard_size=2000), '../json_data/cnndm.valid.0.bert.pt')]\u001b[0m\n",
      "\u001b[34m[2020-04-16 16:06:01,457 INFO] loading vocabulary file ../json_data/vocab.txt\u001b[0m\n",
      "\u001b[34m[2020-04-16 16:06:01,484 INFO] Processing ../json_data/cnndm.valid.0.json\u001b[0m\n",
      "\u001b[34m[2020-04-16 16:06:01,491 INFO] Saving to ../json_data/cnndm.valid.0.bert.pt\u001b[0m\n",
      "\u001b[34m[('../json_data/cnndm.test.0.json', Namespace(dataset='', log_file='../logs/preprocess.log', lower=True, map_path='../data/', max_nsents=100, max_src_ntokens=200, min_nsents=3, min_src_ntokens=5, mode='format_to_bert', n_cpus=4, oracle_mode='greedy', raw_path='../json_data', save_path='../json_data', shard_size=2000), '../json_data/cnndm.test.0.bert.pt')]\u001b[0m\n",
      "\u001b[34m[2020-04-16 16:06:01,567 INFO] loading vocabulary file ../json_data/vocab.txt\u001b[0m\n",
      "\u001b[34m[2020-04-16 16:06:01,595 INFO] Processing ../json_data/cnndm.test.0.json\u001b[0m\n",
      "\u001b[34m[2020-04-16 16:06:01,600 INFO] Saving to ../json_data/cnndm.test.0.bert.pt\u001b[0m\n",
      "\u001b[34mlook at me: ../json_data/cnndm test\u001b[0m\n",
      "\u001b[34mfiles found :  ['../json_data/cnndm.test.0.bert.pt']\u001b[0m\n",
      "\u001b[34mgpu_rank 0\u001b[0m\n",
      "\u001b[34m------------inside train.py test_iter---------------- <models.data_loader.Dataloader object at 0x7ffb0e3aba58>\u001b[0m\n",
      "\u001b[34m---------------sent_scores-------------- tensor([[0.0040, 0.2936, 0.2354, 0.3605, 0.2159, 0.2065, 0.2098, 0.1179, 0.0494,\n",
      "         0.0400, 0.0440]])\u001b[0m\n",
      "\u001b[34m---------------mask--------------------- tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.uint8)\u001b[0m\n",
      "\u001b[34m-----------------inside train.py pred------------------ ['model checking and theorem proving : theorem proving avoids the state explosion problem , but relies heavily on human guidance and tends to get unmanageable for large designs .<q>considerable progress has been made in the last two decades , and many major companies are now using model checking .<q>to extend the potential of the method , this project pursues several avenues of research that will enable larger hardware systems and certain software systems to be verified .']\u001b[0m\n",
      "\u001b[32m[('../json_data/cnndm.valid.0.json', Namespace(dataset='', log_file='../logs/preprocess.log', lower=True, map_path='../data/', max_nsents=100, max_src_ntokens=200, min_nsents=3, min_src_ntokens=5, mode='format_to_bert', n_cpus=4, oracle_mode='greedy', raw_path='../json_data', save_path='../json_data', shard_size=2000), '../json_data/cnndm.valid.0.bert.pt')]\u001b[0m\n",
      "\u001b[32m[2020-04-16 16:06:01,457 INFO] loading vocabulary file ../json_data/vocab.txt\u001b[0m\n",
      "\u001b[32m[2020-04-16 16:06:01,484 INFO] Processing ../json_data/cnndm.valid.0.json\u001b[0m\n",
      "\u001b[32m[2020-04-16 16:06:01,491 INFO] Saving to ../json_data/cnndm.valid.0.bert.pt\u001b[0m\n",
      "\u001b[32m[('../json_data/cnndm.test.0.json', Namespace(dataset='', log_file='../logs/preprocess.log', lower=True, map_path='../data/', max_nsents=100, max_src_ntokens=200, min_nsents=3, min_src_ntokens=5, mode='format_to_bert', n_cpus=4, oracle_mode='greedy', raw_path='../json_data', save_path='../json_data', shard_size=2000), '../json_data/cnndm.test.0.bert.pt')]\u001b[0m\n",
      "\u001b[32m[2020-04-16 16:06:01,567 INFO] loading vocabulary file ../json_data/vocab.txt\u001b[0m\n",
      "\u001b[32m[2020-04-16 16:06:01,595 INFO] Processing ../json_data/cnndm.test.0.json\u001b[0m\n",
      "\u001b[32m[2020-04-16 16:06:01,600 INFO] Saving to ../json_data/cnndm.test.0.bert.pt\u001b[0m\n",
      "\u001b[32mlook at me: ../json_data/cnndm test\u001b[0m\n",
      "\u001b[32mfiles found :  ['../json_data/cnndm.test.0.bert.pt']\u001b[0m\n",
      "\u001b[32mgpu_rank 0\u001b[0m\n",
      "\u001b[32m------------inside train.py test_iter---------------- <models.data_loader.Dataloader object at 0x7ffb0e3aba58>\u001b[0m\n",
      "\u001b[32m---------------sent_scores-------------- tensor([[0.0040, 0.2936, 0.2354, 0.3605, 0.2159, 0.2065, 0.2098, 0.1179, 0.0494,\n",
      "         0.0400, 0.0440]])\u001b[0m\n",
      "\u001b[32m---------------mask--------------------- tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.uint8)\u001b[0m\n",
      "\u001b[32m-----------------inside train.py pred------------------ ['model checking and theorem proving : theorem proving avoids the state explosion problem , but relies heavily on human guidance and tends to get unmanageable for large designs .<q>considerable progress has been made in the last two decades , and many major companies are now using model checking .<q>to extend the potential of the method , this project pursues several avenues of research that will enable larger hardware systems and certain software systems to be verified .']\u001b[0m\n",
      "\u001b[34m-----------------inside train.py gold------------------ ['no summary found']\u001b[0m\n",
      "\u001b[34m<_io.TextIOWrapper name='../json_data/results/_step50000.candidate' mode='w' encoding='ANSI_X3.4-1968'>\u001b[0m\n",
      "\u001b[34m-----combined----- model checking and theorem proving : theorem proving avoids the state explosion problem , but relies heavily on human guidance and tends to get unmanageable for large designs .<q>considerable progress has been made in the last two decades , and many major companies are now using model checking .<q>to extend the potential of the method , this project pursues several avenues of research that will enable larger hardware systems and certain software systems to be verified .\n",
      "\u001b[0m\n",
      "\u001b[34m---------------intermediate---------- model checking and theorem proving : theorem proving avoids the state explosion problem , but relies heavily on human guidance and tends to get unmanageable for large designs .<q>considerable progress has been made in the last two decades , and many major companies are now using model checking .<q>to extend the potential of the method , this project pursues several avenues of research that will enable larger hardware systems and certain software systems to be verified .\n",
      "\u001b[0m\n",
      "\u001b[34m------------in pretty summary----------------- model checking and theorem proving : theorem proving avoids the state explosion problem , but relies heavily on human guidance and tends to get unmanageable for large designs .<q>considerable progress has been made in the last two decades , and many major companies are now using model checking .<q>to extend the potential of the method , this project pursues several avenues of research that will enable larger hardware systems and certain software systems to be verified .\n",
      "\u001b[0m\n",
      "\u001b[34m------------------result------------- Model checking and theorem proving : theorem proving avoids the state explosion problem , but relies heavily on human guidance and tends to get unmanageable for large designs. Considerable progress has been made in the last two decades , and many major companies are now using model checking. To extend the potential of the method , this project pursues several avenues of research that will enable larger hardware systems and certain software systems to be verified.\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [16/Apr/2020 16:06:02] \"#033[37mPOST /invocations HTTP/1.1#033[0m\" 200 -\u001b[0m\n",
      "\u001b[32m-----------------inside train.py gold------------------ ['no summary found']\u001b[0m\n",
      "\u001b[32m<_io.TextIOWrapper name='../json_data/results/_step50000.candidate' mode='w' encoding='ANSI_X3.4-1968'>\u001b[0m\n",
      "\u001b[32m-----combined----- model checking and theorem proving : theorem proving avoids the state explosion problem , but relies heavily on human guidance and tends to get unmanageable for large designs .<q>considerable progress has been made in the last two decades , and many major companies are now using model checking .<q>to extend the potential of the method , this project pursues several avenues of research that will enable larger hardware systems and certain software systems to be verified .\n",
      "\u001b[0m\n",
      "\u001b[32m---------------intermediate---------- model checking and theorem proving : theorem proving avoids the state explosion problem , but relies heavily on human guidance and tends to get unmanageable for large designs .<q>considerable progress has been made in the last two decades , and many major companies are now using model checking .<q>to extend the potential of the method , this project pursues several avenues of research that will enable larger hardware systems and certain software systems to be verified .\n",
      "\u001b[0m\n",
      "\u001b[32m------------in pretty summary----------------- model checking and theorem proving : theorem proving avoids the state explosion problem , but relies heavily on human guidance and tends to get unmanageable for large designs .<q>considerable progress has been made in the last two decades , and many major companies are now using model checking .<q>to extend the potential of the method , this project pursues several avenues of research that will enable larger hardware systems and certain software systems to be verified .\n",
      "\u001b[0m\n",
      "\u001b[32m------------------result------------- Model checking and theorem proving : theorem proving avoids the state explosion problem , but relies heavily on human guidance and tends to get unmanageable for large designs. Considerable progress has been made in the last two decades , and many major companies are now using model checking. To extend the potential of the method , this project pursues several avenues of research that will enable larger hardware systems and certain software systems to be verified.\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [16/Apr/2020 16:06:02] \"#033[37mPOST /invocations HTTP/1.1#033[0m\" 200 -\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch Transform complete\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import uuid\n",
    "transformer = model.transformer(2, 'ml.m5.xlarge')\n",
    "transformer.output_path = \"s3://nsf-bucket/summerized-abstract\"\n",
    "transformer.transform(top1_txt, content_type='text/plain')\n",
    "transformer.wait()\n",
    "print(\"Batch Transform complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YThSIa-ehnGX"
   },
   "source": [
    "## Output from Batch Transform\n",
    "\n",
    "Note: Ensure that the following package is installed on the local system : boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOehVNlDhnGX",
    "outputId": "1dda1784-b3a7-4a4b-9820-955c43c55fbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://nsf-bucket/summerized-abstract\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "print(transformer.output_path)\n",
    "bucketFolder = transformer.output_path.rsplit('/')[3]\n",
    "#print(s3bucket,s3prefix)\n",
    "s3_conn = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IelofWEDhnGZ",
    "outputId": "c80e27bc-c3be-4fa7-d20c-1cdfb0b48481"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file loaded from bucket\n"
     ]
    }
   ],
   "source": [
    "bucket_name=\"nsf-bucket\"\n",
    "with open('result.txt', 'wb') as f:\n",
    "    s3_conn.download_fileobj(bucket_name,bucketFolder+'/top1.txt.out', f)\n",
    "    print(\"Output file loaded from bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ShU-mX2fhnGb"
   },
   "outputs": [],
   "source": [
    "with open('./result.txt', 'rb') as file_stream:\n",
    "    output_text = file_stream.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-2Ydu5tPhnGc"
   },
   "source": [
    "#### Original Text Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lC20mx4MhnGd",
    "outputId": "3bdd6ae8-4deb-49ea-9b68-d04e5aa8062b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This study utilizes publicly available data from the National Science Foundation (NSF) Web Application Programming Interface (API). In this paper, various machine learning techniques are demonstrated to explore, analyze and recommend similar proposal abstracts to aid the NSF or Awardee with the Merit Review Process. These techniques extract textual context and group it with similar context. The goal of the analysis was to utilize a Doc2Vec unsupervised learning algorithms to embed NSF funding proposal abstracts text into vector space.  Once vectorized, the abstracts were grouped together using K-means clustering. These techniques together proved to be successful at grouping similar proposals together and could be used to find similar proposals to newly submitted NSF funding proposals. \r\n",
      "To perform text analysis, SAS® University Edition is used which supports SASPy, SAS® Studio and Python JupyterLab. Gensim Doc2vec is used to generate document vectors for proposal abstracts. Afterwards, document vectors were used to cluster similar abstracts using SAS® Studio KMeans Clustering Module. For visualization, the abstract embeddings were reduced to two dimensions using Principal Component Analysis (PCA) within SAS® Studio. This was then compared to a t-Distributed Stochastic Neighbor Embedding (t-SNE) dimensionality reduction technique as part of the Scikit-learn machine learning toolkit for Python.\r\n",
      "Conclusively, NSF proposal abstract text analysis can help an awardee read and improve their proposal model by identifying similar proposal abstracts from the last 24 years. It could also help NSF evaluators identify similar existing proposals that indirectly provides insights on whether a new proposal is going to be fruitful or not.\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UL2o6hDqhnGf"
   },
   "source": [
    "#### Similar abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Fnb5-1ShnGg",
    "outputId": "b86d3fe5-feed-4588-c3e7-33520c8ca5f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file loaded from bucket\n"
     ]
    }
   ],
   "source": [
    "bucket_name=\"nsf-bucket\"\n",
    "with open('top1.txt', 'wb') as f:\n",
    "    s3_conn.download_fileobj(bucket_name,'top1.txt', f)\n",
    "    print(\"Output file loaded from bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pyr6R3KXhnGi"
   },
   "outputs": [],
   "source": [
    "with open('./top1.txt', 'rb') as file_stream:\n",
    "    top1_text = file_stream.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "je59I3LQhnGj",
    "outputId": "882fc12a-fead-4bcf-c6cf-6130f0dd9a4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ccr-0098072\n",
      "edmund clarke\n",
      "cmu\n",
      "\n",
      "abstract:\n",
      "\n",
      "model checking is an automatic verification technique for concurrent systems such as sequential circuit design and communication protocols in which temporal logic specifications are checked by an exhaustive search of the state space of the concurrent system. considerable progress has been made in the last two decades, and many major companies are now using model checking. to extend the potential of the method, this project pursues several avenues of research that will enable larger hardware systems and certain software systems to be verified.\n",
      "\n",
      "model checking and theorem proving:  theorem proving avoids the state explosion problem, but relies heavily on human guidance and tends to get unmanageable for large designs.  the project attempts to find and implement a practical methodology that will combine the benefits of theorem proving and model checking and apply it to verification of processors and security protocols.\n",
      "\n",
      "sat-based model checking: developed recently as a complementary approach to traditional model checking based on binary decision diagrams (bdds), sat solvers tend to suffer less from the state explosion problem than bdds.  this project investigates how sat solvers and bdd techniques can be further integrated to enable verification of larger systems.\n",
      "\n",
      "software model checking:  although the major successes of model checking have been in hardware, the procedure was originally developed for software.  the first paper by clarke and emerson in 1981 proposed extracting the synchronization skeleton of a concurrent program and model checking it.  advances in model checking have generated renewed interest in this approach.  this project will explore how to achieve this goal.\n"
     ]
    }
   ],
   "source": [
    "print(top1_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DyNXd1R8hnGl"
   },
   "source": [
    "#### Summary of similar abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bLaDnrJjhnGm",
    "outputId": "563a5a21-492f-42a1-ec89-8de7dbbf2e6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checking and theorem proving : theorem proving avoids the state explosion problem , but relies heavily on human guidance and tends to get unmanageable for large designs. Considerable progress has been made in the last two decades , and many major companies are now using model checking. To extend the potential of the method , this project pursues several avenues of research that will enable larger hardware systems and certain software systems to be verified.\n",
      "Execution time : 1.55seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4aIePLvhnGr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j9ymEMqQhnGt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NSFFinal.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
