{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x7ga2OGQC220"
   },
   "source": [
    "# Natural Language Processing using Doc2Vec on National Science Foundation Awards Abstracts\n",
    "---\n",
    "### Team:  \n",
    "Jacob Noble  \n",
    "Himanshu Gamit  \n",
    "Shantanu Hadap\n",
    "\n",
    "\n",
    "## 1. Pre-requisites: Sagemaker Jupyter Instance- ml.m5.xlarge and MphasisDeepInsightsTextSummarizer Product ARN\n",
    "\n",
    "1. Make sure you are in North Virginia Region.\n",
    "2. Go to https://aws.amazon.com/marketplace/pp/prodview-uzkcdmjuagetk and \n",
    "3. Subscribe and accept offer.\n",
    "3. Wait for a confirmation email and then Continue to Configuration.\n",
    "4. Select your launch method to SageMaker Console\n",
    "5. Go to Configure for Amazon SageMaker console\n",
    "6. Copy Prodcut arn keep it for later use. \n",
    "\n",
    "\n",
    "#### Make Sure to update product arn as highlighted below \n",
    "\n",
    "#### MphasisDeepInsightsTextSummarizer Product ARN\n",
    "\n",
    "## 2. FOR CUSTOM USER ABSTRACT INPUT Update text variable section highlighted below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdOYaCrjC22-"
   },
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from gensim) (1.16.4)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from gensim) (1.11.1)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (1.12.24)\n",
      "Requirement already satisfied: boto in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.48.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.20.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.24 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.15.24)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.24->boto3->smart-open>=1.8.1->gensim) (2.7.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.24->boto3->smart-open>=1.8.1->gensim) (0.14)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "12pVWFUTC22_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler # Replace with SAS version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "HiqBFGEx4kNQ",
    "outputId": "29685a3c-3835-490e-a866-b2f6d25ff668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-16 18:15:29--  https://nsfdata.s3.amazonaws.com/nsfdataset.zip\n",
      "Resolving nsfdata.s3.amazonaws.com (nsfdata.s3.amazonaws.com)... 52.217.39.20\n",
      "Connecting to nsfdata.s3.amazonaws.com (nsfdata.s3.amazonaws.com)|52.217.39.20|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 358656602 (342M) [application/zip]\n",
      "Saving to: ‘nsfdataset.zip’\n",
      "\n",
      "nsfdataset.zip      100%[===================>] 342.04M  31.5MB/s    in 8.7s    \n",
      "\n",
      "2020-04-16 18:15:38 (39.5 MB/s) - ‘nsfdataset.zip’ saved [358656602/358656602]\n",
      "\n",
      "Archive:  nsfdataset.zip\n",
      "  inflating: data/nsf_proposals.csv  \n"
     ]
    }
   ],
   "source": [
    "#!rm -r data\n",
    "!wget https://nsfdata.s3.amazonaws.com/nsfdataset.zip -O nsfdataset.zip\n",
    "!unzip -o nsfdataset.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsf_proposals.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "ELme3JwFr579",
    "outputId": "60c0c11b-9150-45a7-a261-d0176933d6b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Projects:  329321 \n",
      "Total Features:  2\n"
     ]
    }
   ],
   "source": [
    "#import saspy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "selected_cols = ['abstractText','date']\n",
    "projects = pd.read_csv(\"data/nsf_proposals.csv\", usecols = selected_cols, low_memory=False) #, nrows=30000\n",
    "print (\"Total Projects: \", projects.shape[0], \"\\nTotal Features: \", projects.shape[1])\n",
    "projects.date = pd.to_datetime(projects.date.str.replace('D', 'T'))\n",
    "projects = projects.sort_values('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPmscOBnC23B"
   },
   "source": [
    "### Load in Data using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "colab_type": "code",
    "id": "WpMB6ablC23C",
    "outputId": "d7dffdab-360b-4070-d863-772f4cd0c5e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of the abstractText: 8166\n",
      "Min length of the abstractText: 1\n",
      "Avg length of the abstractText: 1645.861842268939\n",
      "Max words abstractText: 1252\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstractText</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>295988</th>\n",
       "      <td>Nondestructive Evaluation (NDE) is important t...</td>\n",
       "      <td>1985-08-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307099</th>\n",
       "      <td>An Industry/University Cooperative Research Ce...</td>\n",
       "      <td>1985-08-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284877</th>\n",
       "      <td>This research seeks to obtain a representation...</td>\n",
       "      <td>1985-09-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177779</th>\n",
       "      <td>This is an attempt to develop a \"solvated elec...</td>\n",
       "      <td>1986-01-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166668</th>\n",
       "      <td>The New York State College of Ceramics at Alfr...</td>\n",
       "      <td>1986-06-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             abstractText       date\n",
       "295988  Nondestructive Evaluation (NDE) is important t... 1985-08-30\n",
       "307099  An Industry/University Cooperative Research Ce... 1985-08-30\n",
       "284877  This research seeks to obtain a representation... 1985-09-06\n",
       "177779  This is an attempt to develop a \"solvated elec... 1986-01-16\n",
       "166668  The New York State College of Ceramics at Alfr... 1986-06-03"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop values without Abstract texts\n",
    "projects = projects.dropna(how='any')\n",
    "\n",
    "print(\"Max length of the abstractText:\", projects.abstractText.str.len().max())\n",
    "print(\"Min length of the abstractText:\", projects.abstractText.str.len().min())\n",
    "print(\"Avg length of the abstractText:\", projects.abstractText.apply(lambda x: len(x) - x.count(\" \")).mean())\n",
    "\n",
    "words = projects.abstractText.str.split().apply(len)\n",
    "print(\"Max words abstractText:\", words.max())\n",
    "projects.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJJZRTyCC23F"
   },
   "source": [
    "### Setup Training Set for Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "puEINuT5C23F",
    "outputId": "ee4bd738-7544-4d88-cac6-7e5fe59d9055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of abstracts:  321560\n"
     ]
    }
   ],
   "source": [
    "X_raw = projects.abstractText.str.lower().values\n",
    "num_of_docs = len(X_raw)\n",
    "print('Number of abstracts: ', num_of_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-16 18:16:18--  https://nsfdata.s3.amazonaws.com/0nsf_doc2v.model\n",
      "Resolving nsfdata.s3.amazonaws.com (nsfdata.s3.amazonaws.com)... 52.216.113.211\n",
      "Connecting to nsfdata.s3.amazonaws.com (nsfdata.s3.amazonaws.com)|52.216.113.211|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19628574 (19M) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘0nsf_doc2v.model’\n",
      "\n",
      "0nsf_doc2v.model    100%[===================>]  18.72M  53.7MB/s    in 0.3s    \n",
      "\n",
      "2020-04-16 18:16:18 (53.7 MB/s) - ‘0nsf_doc2v.model’ saved [19628574/19628574]\n",
      "\n",
      "--2020-04-16 18:16:19--  https://nsfdata.s3.amazonaws.com/0nsf_doc2v.model.docvecs.vectors_docs.npy\n",
      "Resolving nsfdata.s3.amazonaws.com (nsfdata.s3.amazonaws.com)... 52.216.164.27\n",
      "Connecting to nsfdata.s3.amazonaws.com (nsfdata.s3.amazonaws.com)|52.216.164.27|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 385872128 (368M) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘0nsf_doc2v.model.docvecs.vectors_docs.npy’\n",
      "\n",
      "0nsf_doc2v.model.do 100%[===================>] 368.00M  37.8MB/s    in 8.7s    \n",
      "\n",
      "2020-04-16 18:16:27 (42.4 MB/s) - ‘0nsf_doc2v.model.docvecs.vectors_docs.npy’ saved [385872128/385872128]\n",
      "\n",
      "--2020-04-16 18:16:28--  https://nsfdata.s3.amazonaws.com/0nsf_doc2v.model.trainables.syn1neg.npy\n",
      "Resolving nsfdata.s3.amazonaws.com (nsfdata.s3.amazonaws.com)... 52.217.37.36\n",
      "Connecting to nsfdata.s3.amazonaws.com (nsfdata.s3.amazonaws.com)|52.217.37.36|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 304117328 (290M) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘0nsf_doc2v.model.trainables.syn1neg.npy’\n",
      "\n",
      "0nsf_doc2v.model.tr 100%[===================>] 290.03M  46.5MB/s    in 6.0s    \n",
      "\n",
      "2020-04-16 18:16:34 (48.4 MB/s) - ‘0nsf_doc2v.model.trainables.syn1neg.npy’ saved [304117328/304117328]\n",
      "\n",
      "--2020-04-16 18:16:35--  https://nsfdata.s3.amazonaws.com/0nsf_doc2v.model.wv.vectors.npy\n",
      "Resolving nsfdata.s3.amazonaws.com (nsfdata.s3.amazonaws.com)... 52.216.200.171\n",
      "Connecting to nsfdata.s3.amazonaws.com (nsfdata.s3.amazonaws.com)|52.216.200.171|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 304117328 (290M) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘0nsf_doc2v.model.wv.vectors.npy’\n",
      "\n",
      "0nsf_doc2v.model.wv 100%[===================>] 290.03M  59.4MB/s    in 4.8s    \n",
      "\n",
      "2020-04-16 18:16:40 (60.0 MB/s) - ‘0nsf_doc2v.model.wv.vectors.npy’ saved [304117328/304117328]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nsfdata.s3.amazonaws.com/0nsf_doc2v.model -O 0nsf_doc2v.model\n",
    "!wget https://nsfdata.s3.amazonaws.com/0nsf_doc2v.model.docvecs.vectors_docs.npy -O 0nsf_doc2v.model.docvecs.vectors_docs.npy\n",
    "!wget https://nsfdata.s3.amazonaws.com/0nsf_doc2v.model.trainables.syn1neg.npy -O 0nsf_doc2v.model.trainables.syn1neg.npy\n",
    "!wget https://nsfdata.s3.amazonaws.com/0nsf_doc2v.model.wv.vectors.npy -O 0nsf_doc2v.model.wv.vectors.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ru1PXGUC23J"
   },
   "source": [
    "### Create generator that will tokenize the training abstracts on the fly to save on memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i1pg5829C23K"
   },
   "outputs": [],
   "source": [
    "def doc_generator(input_docs_array):\n",
    "    for i, doc in enumerate(input_docs_array):\n",
    "        tokens = gensim.utils.simple_preprocess(doc)\n",
    "        yield gensim.models.doc2vec.TaggedDocument(tokens, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t7ZvV6OTC23Q"
   },
   "outputs": [],
   "source": [
    "X = doc_generator(X_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHy9VORiC23V"
   },
   "source": [
    "### Create Initial Doc2Vec Model and Training\n",
    "\n",
    "#### Skip to next section to work load in a pretrained model.  Training could take a potentially long time.\n",
    "\n",
    "We conduct the replication to Document Embedding with Paragraph Vectors (http://arxiv.org/abs/1507.07998). In this paper, they showed only DBOW results to NSF data. So we replicate this experiments using not only DBOW but also DM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "ArrJv_dvC23W",
    "outputId": "e5ac4c70-8b29-4bc6-de7f-7dc1b252a475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 10 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#import multiprocessing\n",
    "#from pprint import pprint\n",
    "#cores = multiprocessing.cpu_count()\n",
    "\n",
    "#It is training 100 epochs in 300 dimension vector space\n",
    "#models = [\n",
    "    # PV-DBOW \n",
    "#    Doc2Vec(dm=0, dbow_words=1, size=300, window=8, min_count=1, iter=100, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    #Doc2Vec(dm=1, dm_mean=1, size=300, window=8, min_count=1, iter =100, workers=cores),\n",
    "#]\n",
    "\n",
    "#models[0].build_vocab(doc_generator(X_raw))\n",
    "#print(str(models[0]))\n",
    "#models[1].reset_from(models[0])\n",
    "#print(str(models[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "c5QCW4n32P7-",
    "outputId": "42212ea2-54a7-4b7c-aa78-95ecb4b90be4"
   },
   "outputs": [],
   "source": [
    "#%%time \n",
    "#for i, model in enumerate(models):\n",
    "#    model.train(doc_generator(X_raw), total_examples=len(X_raw), epochs=model.iter)\n",
    "#    model.save(str(i)+'nsf_doc2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0BS4x08Y4T-_",
    "outputId": "c6dddc6b-acd5-4dd6-ef0d-e692df17dd1b"
   },
   "outputs": [],
   "source": [
    "#print(\"Doc2Vec Embedding Shape:\",model.docvecs.vectors_docs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mr5BKOvDC23c"
   },
   "outputs": [],
   "source": [
    "# model.train(doc_generator(X_raw), total_examples=num_of_docs, epochs=1)\n",
    "# model.save('nsf_doc2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gs7Qc0KOC23h"
   },
   "source": [
    "### Load Existing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "hJbB9-JzC23k",
    "outputId": "fc10c443-1d21-48cc-cb81-cee045bae332"
   },
   "outputs": [],
   "source": [
    "existing_model =  '0nsf_doc2v.model' #'nsf_d2v_100.model' # Name of Existing model to load\n",
    "model = gensim.models.Doc2Vec.load(str(existing_model)) # Model is assumed to be in the shared folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "slapVISDC23n"
   },
   "source": [
    "### Testing Doc2Vec Model\n",
    "\n",
    "1. Infer Embedding Vecor for a New Abstract\n",
    "2. Generating Most Similar Articles based on document cluster. This is using cosine similary score.\n",
    "3. Storing the most similar files to S3 bucket.\n",
    "\n",
    "\n",
    "#### 1. Infer Embedding Vecor for a New Abstract\n",
    "\n",
    "First, calculating cosine similarity of New Text using Paragraph Vector. Word Vector and Document Vector are separately stored. We have to add .docvecs after model name to extract Document Vector from Doc2Vec Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "w_o5Knh0i6Qk",
    "outputId": "d102ee6c-25f1-487f-a2b2-02f3a15bf574"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-16 18:16:44--  https://nsfdata.s3.amazonaws.com/input.txt\n",
      "Resolving nsfdata.s3.amazonaws.com (nsfdata.s3.amazonaws.com)... 52.216.92.211\n",
      "Connecting to nsfdata.s3.amazonaws.com (nsfdata.s3.amazonaws.com)|52.216.92.211|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1758 (1.7K) [text/plain]\n",
      "Saving to: ‘input.txt.3’\n",
      "\n",
      "input.txt.3         100%[===================>]   1.72K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-04-16 18:16:44 (113 MB/s) - ‘input.txt.3’ saved [1758/1758]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nsfdata.s3.amazonaws.com/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iEDLXN2-jSmj"
   },
   "outputs": [],
   "source": [
    "with open('./input.txt', 'rb') as file_stream:\n",
    "    text = file_stream.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR CUSTOM USER ABSTRACT INPUT - Update below text variable value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "D4T1hmbghnFx",
    "outputId": "5e310518-7b6c-41b1-b70d-4bf99915d4c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This study utilizes publicly available data from the National Science Foundation (NSF) Web Application Programming Interface (API). In this paper, various machine learning techniques are demonstrated to explore, analyze and recommend similar proposal abstracts to aid the NSF or Awardee with the Merit Review Process. These techniques extract textual context and group it with similar context. The goal of the analysis was to utilize a Doc2Vec unsupervised learning algorithms to embed NSF funding proposal abstracts text into vector space.  Once vectorized, the abstracts were grouped together using K-means clustering. These techniques together proved to be successful at grouping similar proposals together and could be used to find similar proposals to newly submitted NSF funding proposals. \r\n",
      "To perform text analysis, SAS® University Edition is used which supports SASPy, SAS® Studio and Python JupyterLab. Gensim Doc2vec is used to generate document vectors for proposal abstracts. Afterwards, document vectors were used to cluster similar abstracts using SAS® Studio KMeans Clustering Module. For visualization, the abstract embeddings were reduced to two dimensions using Principal Component Analysis (PCA) within SAS® Studio. This was then compared to a t-Distributed Stochastic Neighbor Embedding (t-SNE) dimensionality reduction technique as part of the Scikit-learn machine learning toolkit for Python.\r\n",
      "Conclusively, NSF proposal abstract text analysis can help an awardee read and improve their proposal model by identifying similar proposal abstracts from the last 24 years. It could also help NSF evaluators identify similar existing proposals that indirectly provides insights on whether a new proposal is going to be fruitful or not.\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#text = \"\"\"This study utilizes publicly available data from the National Science Foundation (NSF) Web Application Programming Interface (API). In this paper, various machine learning techniques are demonstrated to explore, analyze and recommend similar proposal abstracts to aid the NSF or Awardee with the Merit Review Process. These techniques extract textual context and group it with similar context. The goal of the analysis was to utilize a Doc2Vec unsupervised learning algorithms to embed NSF funding proposal abstracts text into vector space.  Once vectorized, the abstracts were grouped together using K-means clustering. These techniques together proved to be successful at grouping similar proposals together and could be used to find similar proposals to newly submitted NSF funding proposals. To perform text analysis, SAS® University Edition is used which supports SASPy, SAS® Studio and Python JupyterLab. Gensim Doc2vec is used to generate document vectors for proposal abstracts. Afterwards, document vectors were used to cluster similar abstracts using SAS® Studio KMeans Clustering Module. For visualization, the abstract embeddings were reduced to two dimensions using Principal Component Analysis (PCA) within SAS® Studio. This was then compared to a t-Distributed Stochastic Neighbor Embedding (t-SNE) dimensionality reduction technique as part of the Scikit-learn machine learning toolkit for Python.Conclusively, NSF proposal abstract text analysis can help an awardee read and improve their proposal model by identifying similar proposal abstracts from the last 24 years. It could also help NSF evaluators identify similar existing proposals that indirectly provides insights on whether a new proposal is going to be fruitful or not.\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9UVA8d2kC23o"
   },
   "outputs": [],
   "source": [
    "test_vector = model.infer_vector(gensim.utils.simple_preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "woPjuZXDC23s",
    "outputId": "a03d413f-1391-4a0d-e9c4-dc4544c227cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.82562828e-01, -2.77844161e-01, -3.82432163e-01, -1.89732462e-01,\n",
       "        6.49394214e-01, -1.13833892e+00, -1.21166408e+00,  8.79729450e-01,\n",
       "        9.96808290e-01, -1.09390342e+00,  7.74606317e-02,  8.12940061e-01,\n",
       "        2.34025121e-01,  1.31931531e+00,  1.04793572e+00,  6.75552785e-01,\n",
       "        5.95528781e-01, -3.19337279e-01,  2.45541856e-01,  8.75829518e-01,\n",
       "        3.35987270e-01, -3.82613391e-01,  1.99713588e-01, -1.15263987e+00,\n",
       "       -6.99651659e-01,  4.36860800e-01,  1.10605597e+00, -3.55357915e-01,\n",
       "        2.16153443e-01, -1.07333824e-01,  1.35348177e+00,  3.09221238e-01,\n",
       "        9.06548277e-02,  2.64715075e-01,  1.61698043e+00, -1.11064404e-01,\n",
       "        1.29366469e+00,  8.58210176e-02,  8.75645638e-01, -1.21472515e-01,\n",
       "       -1.05213082e+00,  4.88076061e-01,  7.85005927e-01,  1.90077424e+00,\n",
       "        1.08122349e+00,  7.67614543e-01,  2.42246002e-01,  9.50537980e-01,\n",
       "       -3.95221673e-02,  5.71659207e-01,  6.59440458e-01, -1.89894646e-01,\n",
       "        4.92445230e-01,  5.91077566e-01, -4.41322625e-01,  2.23606661e-01,\n",
       "        1.63924813e-01, -1.08860040e+00, -5.40610492e-01, -6.23594165e-01,\n",
       "        5.23900509e-01,  1.34223783e+00, -1.08035654e-01, -1.29695618e+00,\n",
       "       -3.47569818e-03,  1.40511975e-01,  1.39932349e-01,  1.97327197e-01,\n",
       "       -7.03734636e-01, -2.30077401e-01, -4.77232158e-01, -4.08074409e-01,\n",
       "        3.56423818e-02,  7.94164762e-02,  4.62738127e-01,  5.12636065e-01,\n",
       "       -9.13884863e-02, -1.97327673e-01, -2.57629275e-01, -6.55050874e-02,\n",
       "       -2.23762482e-01,  1.33284539e-01, -6.98792338e-01, -1.51831970e-01,\n",
       "        1.05314350e+00, -3.46514136e-02,  2.32918456e-01, -1.68293044e-02,\n",
       "       -3.95127237e-01,  5.10836422e-01, -4.69778150e-01, -4.28106546e-01,\n",
       "       -4.62696552e-02,  1.27474377e-02,  4.14662659e-01,  7.19606519e-01,\n",
       "       -1.70846879e-02, -6.70802414e-01, -4.02542412e-01,  4.61599380e-01,\n",
       "        3.65820169e-01,  2.83307612e-01, -2.85504013e-01, -5.09874165e-01,\n",
       "       -3.37927133e-01, -1.88086495e-01,  2.57853746e-01, -5.57222307e-01,\n",
       "       -6.72834218e-02, -1.14946282e+00, -2.83049613e-01, -6.56876683e-01,\n",
       "        2.91030198e-01, -7.52556503e-01,  1.31400108e+00, -5.06536901e-01,\n",
       "       -8.50884020e-01,  6.41634405e-01, -4.02520806e-01, -1.82258153e+00,\n",
       "        2.82132357e-01, -8.51704553e-02, -9.26311135e-01,  4.78757501e-01,\n",
       "       -6.04346511e-04, -8.13005865e-01, -1.01880622e+00, -3.17546278e-01,\n",
       "        3.88554260e-02, -7.55802035e-01,  2.33057827e-01,  1.90218374e-01,\n",
       "       -3.01141202e-01,  5.50032735e-01,  9.26186025e-01, -6.26920521e-01,\n",
       "       -4.30215091e-01, -5.40556312e-02,  7.24188030e-01,  1.14512309e-01,\n",
       "        4.41639215e-01, -7.87238121e-01,  5.46492159e-01,  5.78700960e-01,\n",
       "        6.16535723e-01, -7.51744926e-01,  1.09019756e+00, -1.20193827e+00,\n",
       "        8.26951489e-02, -1.58103500e-02,  9.83100653e-01,  1.01962805e+00,\n",
       "       -3.40819657e-01,  8.81693065e-02,  6.68738782e-02,  4.81661171e-01,\n",
       "        1.33482170e+00, -8.36572275e-02,  2.95401573e-01,  6.64233685e-01,\n",
       "       -4.35611218e-01, -4.24548090e-01, -7.29918957e-01, -1.23279721e-01,\n",
       "       -5.31080067e-01, -1.30346954e+00,  1.05714452e+00,  3.10936093e-01,\n",
       "       -1.04201484e+00,  4.59336787e-01,  1.25478446e-01, -6.07896268e-01,\n",
       "       -3.25089514e-01,  1.06224060e-01,  1.94916241e-02, -8.21349695e-02,\n",
       "       -8.83328080e-01, -5.80659688e-01,  4.26360779e-03, -1.07083330e-02,\n",
       "       -8.27800155e-01, -7.11545050e-01,  5.63934267e-01,  8.69043231e-01,\n",
       "       -2.98447788e-01,  5.29346287e-01, -1.75590694e+00,  4.10514593e-01,\n",
       "       -1.56983614e-01,  5.20615399e-01, -3.07629615e-01, -5.63367233e-02,\n",
       "       -1.48558483e-01,  8.60329092e-01,  4.06380713e-01, -1.33763747e-02,\n",
       "        2.57813156e-01,  7.09905088e-01,  3.24893355e-01, -1.11367106e+00,\n",
       "        5.05798399e-01,  6.30719244e-01,  4.03735459e-01,  4.03680921e-01,\n",
       "       -7.94237200e-03,  5.16303480e-01,  3.45424920e-01,  4.27216381e-01,\n",
       "        6.93245173e-01, -2.50335306e-01,  1.26243567e+00,  9.29510370e-02,\n",
       "       -1.23153850e-01, -1.00689101e+00, -3.82342190e-01,  6.83259726e-01,\n",
       "        5.17714508e-02, -2.44856730e-01,  1.74025837e-02, -1.05082631e+00,\n",
       "       -7.50537276e-01,  4.50578243e-01, -9.56574559e-01,  1.86172083e-01,\n",
       "        7.40180314e-01, -1.03979862e+00, -9.32970583e-01,  4.23490554e-01,\n",
       "        1.44148007e-01, -1.12160251e-01, -1.15739191e+00,  7.59823859e-01,\n",
       "        2.00885519e-01,  1.71427041e-01,  1.11929066e-01, -1.05691385e+00,\n",
       "       -5.38495779e-01, -4.13849391e-02,  4.62751180e-01, -1.87215060e-01,\n",
       "        5.61250627e-01, -1.49171197e+00, -1.96310937e+00, -5.03976703e-01,\n",
       "       -1.41397417e+00,  2.26435229e-01, -7.11536109e-01, -7.04447031e-01,\n",
       "       -1.62865534e-01,  4.10648674e-01, -7.17735946e-01,  2.61047554e+00,\n",
       "       -1.07770395e+00,  1.00056541e+00,  2.51310855e-01,  5.33453703e-01,\n",
       "        1.12663913e+00, -2.14433044e-01,  9.50660825e-01,  1.14928472e+00,\n",
       "        5.40479600e-01, -1.03307998e+00,  7.18147218e-01, -3.52878243e-01,\n",
       "        7.20409095e-01, -6.47528172e-01, -7.52319574e-01,  8.83563533e-02,\n",
       "        1.43556908e-01, -4.89089996e-01,  3.60929444e-02,  3.80687714e-01,\n",
       "        4.59187090e-01, -8.13922510e-02, -9.97287035e-01,  5.54791331e-01,\n",
       "       -8.65961015e-01, -3.55704576e-01,  2.96230227e-01, -1.76395640e-01,\n",
       "       -3.13876182e-01, -1.19647816e-01, -1.60458222e-01,  5.57846367e-01,\n",
       "       -1.74701169e-01,  1.16149589e-01,  1.50293303e+00, -2.91119292e-02,\n",
       "       -4.21994716e-01,  3.25020671e-01, -1.14670968e+00,  3.46864730e-01,\n",
       "        1.05426466e+00, -3.74827296e-01,  2.15434060e-02, -1.04851830e+00,\n",
       "       -4.89701509e-01,  6.10159159e-01,  1.51713669e-01,  6.91801012e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ukx9sWJfjCa"
   },
   "source": [
    "#### 2. Generating Most Similar Articles based on document cluster. This is using cosine similary score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "MZ8dZDfPC23v",
    "outputId": "d336034e-4a84-4b74-f430-9dad9bd712ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(281437, 0.27616167068481445), (277163, 0.266385555267334), (119952, 0.2662033438682556)]\n"
     ]
    }
   ],
   "source": [
    "similar = model.docvecs.most_similar(\n",
    "    positive=[test_vector], \n",
    "    topn=3)\n",
    "print(similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'intelligent systems that say, recommend music or movies based on past interests, or recognize faces or handwriting based on labeled samples, often learn from examples using \"supervised learning.\"  the system tries to find a prediction function: a combination of feature values of the song, movie, image, or pen movements, that, on known inputs, produces score values that agree with known preferences. some combinations may add with simple positive or negative weight parameters (the more guitar the better, or i really don\\'t want accordion), while others can be more complex (nether too loud nor too soft).  if parameters for such a function can be found, then it can be hoped that, on a new input, the function will be a good approximation for the preference. \\n\\nin scientific computing, there are many optimization techniques used to find the best parameters.  the type called \"gradient methods\" is like a group hike that gets caught in the hills after dark; the members want to go downhill to return to the valley quickly, but take small steps so as not to trip. with a little light, the group can discover more about its vicinity to 1) suggest the best direction, 2) take longer steps without tripping, or 3) send different members in different directions so that someone finds the best way. when there are many parameters (not just latitude and longitude) there are many more directions to step.  simple combinations define simple (aka convex) valleys, and many optimization-based learning methods (including support vector machines (svm),  least squares, and logistic regression) have been effectively applied to find the best parameters.  more complex combinations that sometime lead to better learning, may define non-convex valleys, so the known methods may get stuck in dips or have to take very small steps -- they often lack theoretical convergence guarantees and do not always work well in practice. \\n\\nthis project will explore non-convex optimization for machine learning with three techniques that are analogous to the hikers? use of the light: \\nfirst, new techniques will be explored for exploiting approximate second-order derivatives within stochastic methods, which is expected to improve performance over stochastic gradient methods, avoid convergence to saddle points, and improve complexity guarantees over first-order approaches. compared to other such techniques that have been proposed, these approaches will be unique as they will be set within trust-region frameworks, the exploration of which represents the second component of the project. known for decades to offer improved performance for nonconvex optimization, trust region algorithms have not fully been explored for machine learning, and we believe that, when combined with second-order information, dramatic improvements (both theoretically and practically) can be achieved. finally, for such methods to be efficient in large-scale settings, one needs to offer techniques for solving trust region subproblems in situations when all data might not be stored on a single computer. to address this,  parallel and distributed optimization techniques will be developed for solving trust region subproblems and related problems.  the three pis work together with about a dozen students at lehigh; their website is one way they disseminate research papers, software, and news of weekly activities. \\n\\nthis project is funded jointly by nsf cise ccf algorithmic foundations, and nsf mps dms computational mathematics.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_raw[similar[1][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yw_EKxCmfzZT"
   },
   "source": [
    "3. Storing the most similar files to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnnZTNlBIWTT"
   },
   "outputs": [],
   "source": [
    "# file = open('top1.txt', 'w',encoding=\"utf-8\")\n",
    "# file.write(X_raw[similar[0][0]])\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WX7zmH6oC23y"
   },
   "outputs": [],
   "source": [
    "# file = open('top2.txt', 'w',encoding=\"utf-8\")\n",
    "# file.write(X_raw[similar[1][0]])\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5FWQ2EF8C24H"
   },
   "outputs": [],
   "source": [
    "# file = open('top3.txt', 'w',encoding=\"utf-8\")\n",
    "# file.write(X_raw[similar[2][0]])\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'DB2E5EBEFE244C97',\n",
       "  'HostId': 'saXGs5Bk+IOuaOruV/FsDrcgJZnr8lOYIkDxeKT9QGAMVd8e2dzdhj+6yjlIikigJuW2RZo71OY=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'saXGs5Bk+IOuaOruV/FsDrcgJZnr8lOYIkDxeKT9QGAMVd8e2dzdhj+6yjlIikigJuW2RZo71OY=',\n",
       "   'x-amz-request-id': 'DB2E5EBEFE244C97',\n",
       "   'date': 'Thu, 16 Apr 2020 18:16:46 GMT',\n",
       "   'etag': '\"0ecd0c3db9b152d84f1a90c6a7bc7cfb\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"0ecd0c3db9b152d84f1a90c6a7bc7cfb\"'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "s3_conn = boto3.client(\"s3\")\n",
    "s3 = boto3.resource('s3')\n",
    "object = s3.Object('nsfdata', 'top1.txt')\n",
    "object.put(Body=X_raw[similar[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2G0n0VRaC24J"
   },
   "source": [
    "###  MphasisDeepInsightsTextSummarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LjVXD-4jhnGK"
   },
   "source": [
    "### Create the session\n",
    "\n",
    "The session remembers our connection parameters to SageMaker. We'll use it to perform all of our SageMaker operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "ThbzqpNFkEeS",
    "outputId": "f816d871-9c3b-4244-cc39-d0105cdc2150"
   },
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sage.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JTTJoGXrhnGM"
   },
   "source": [
    "## Create Model\n",
    "\n",
    "Now we use the Model Package to create a model,\n",
    "\n",
    "# PLEASE CHANGE MphasisDeepInsightsTextSummarizer Product ARN\n",
    "\n",
    "## Make sure you update model_package_arn to product arn which you retrieved from marketplace-text-summarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0VdfpjcehnGN"
   },
   "outputs": [],
   "source": [
    "# Please use the appropriate ARN obtained after subscribing to the model to define 'model_package_arn'\n",
    "##OHIO REGION\n",
    "#model_package_arn = 'arn:aws:sagemaker:us-east-2:057799348421:model-package/marketplace-text-summarizer-11-d2490248e8de20f24ae3b72d0d74654c'\n",
    "\n",
    "##North Virginia Region\n",
    "model_package_arn = 'arn:aws:sagemaker:us-east-1:865070037744:model-package/marketplace-text-summarizer-11-d2490248e8de20f24ae3b72d0d74654c'\n",
    "\n",
    "from sagemaker import ModelPackage\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "sagemaker_session = sage.Session()\n",
    "model = ModelPackage(model_package_arn=model_package_arn,\n",
    "                    role = role,\n",
    "                    sagemaker_session = sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YPPl9wQ-hnGP"
   },
   "source": [
    "## Input File\n",
    "\n",
    "Now we pull a sample input file for testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uRE7YW7yhnGQ"
   },
   "outputs": [],
   "source": [
    "top1_txt=\"s3://nsfdata/top1.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7rBN7cQLhnGU"
   },
   "source": [
    "## Batch Transform Job\n",
    "\n",
    "Now let's use the model built to run a batch inference job and verify it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ul_Wsbb1hnGV",
    "outputId": "7d3504fe-061d-4179-c793-350ad57e0cb7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................\n",
      ".\u001b[35m2020-04-16T18:22:14.369:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[36m2020-04-16T18:22:14.369:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m * Serving Flask app \"serve\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n",
      " * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [16/Apr/2020 18:22:14] \"#033[37mGET /ping HTTP/1.1#033[0m\" 200 -\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [16/Apr/2020 18:22:14] \"#033[33mGET /execution-parameters HTTP/1.1#033[0m\" 404 -\u001b[0m\n",
      "\u001b[32m * Serving Flask app \"serve\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n",
      " * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [16/Apr/2020 18:22:14] \"#033[37mGET /ping HTTP/1.1#033[0m\" 200 -\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [16/Apr/2020 18:22:14] \"#033[33mGET /execution-parameters HTTP/1.1#033[0m\" 404 -\u001b[0m\n",
      "\u001b[34m2020-04-16T18:22:13.209:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[33m * Serving Flask app \"serve\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n",
      " * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)\u001b[0m\n",
      "\u001b[33m169.254.255.130 - - [16/Apr/2020 18:22:13] \"#033[37mGET /ping HTTP/1.1#033[0m\" 200 -\u001b[0m\n",
      "\u001b[33m169.254.255.130 - - [16/Apr/2020 18:22:13] \"#033[33mGET /execution-parameters HTTP/1.1#033[0m\" 404 -\u001b[0m\n",
      "\u001b[33m---input--- brown university proposes a project to create a flexible, language agnostic software library that makes text and block programming tools accessible to visually-impaired users.  screen-readers, the most common affordable tool for presenting text to those who can't read it directly, have difficulties presenting source code which is symbol-heavy and navigated best not as a list of words, but as an abstract syntax tree (ast). this project will produce a collection of javascript libraries that annotate source code with structural descriptions suitable for presentation by screen readers. \n",
      "\u001b[0m\n",
      "\u001b[33mjavascript was chosen because web-based environments address common technical and logistical challenges in schools. they place fewer demands on it staff for installation and maintenance, and enable students to work outside of school without manually transferring files. web-based software is also easier to upgrade and evolve on the developer's end. being the lingua-franca for cloud-based uis, the browser's document object model (dom) is currently receiving significant attention when it comes to accessibility and thus, by using the dom to display our ide, we can leverage these investments now and in the future. the libraries created in this project will be used to make two web-based environments--wescheme and code.pyret.org--accessible, and those environments will be used in conducting usability testing with visually-impaired users.\u001b[0m\n",
      "\u001b[33m----in bert text to json--------- [{'src': [['brown', 'university', 'proposes', 'a', 'project', 'to', 'create', 'a', 'flexible', ',', 'language', 'agnostic', 'software', 'library', 'that', 'makes', 'text', 'and', 'block', 'programming', 'tools', 'accessible', 'to', 'visually-impaired', 'users', '.'], ['screen-readers', ',', 'the', 'most', 'common', 'affordable', 'tool', 'for', 'presenting', 'text', 'to', 'those', 'who', 'ca', \"n't\", 'read', 'it', 'directly', ',', 'have', 'difficulties', 'presenting', 'source', 'code', 'which', 'is', 'symbol-heavy', 'and', 'navigated', 'best', 'not', 'as', 'a', 'list', 'of', 'words', ',', 'but', 'as', 'an', 'abstract', 'syntax', 'tree', '(', 'ast', ')', '.'], ['this', 'project', 'will', 'produce', 'a', 'collection', 'of', 'javascript', 'libraries', 'that', 'annotate', 'source', 'code', 'with', 'structural', 'descriptions', 'suitable', 'for', 'presentation', 'by', 'screen', 'readers', '.'], ['javascript', 'was', 'chosen', 'because', 'web-based', 'environments', 'address', 'common', 'technical', 'and', 'logistical', 'challenges', 'in', 'schools', '.'], ['they', 'place', 'fewer', 'demands', 'on', 'it', 'staff', 'for', 'installation', 'and', 'maintenance', ',', 'and', 'enable', 'students', 'to', 'work', 'outside', 'of', 'school', 'without', 'manually', 'transferring', 'files', '.'], ['web-based', 'software', 'is', 'also', 'easier', 'to', 'upgrade', 'and', 'evolve', 'on', 'the', 'developer', \"'s\", 'end', '.'], ['being', 'the', 'lingua-franca', 'for', 'cloud-based', 'uis', ',', 'the', 'browser', \"'s\", 'document', 'object', 'model', '(', 'dom', ')', 'is', 'currently', 'receiving', 'significant', 'attention', 'when', 'it', 'comes', 'to', 'accessibility', 'and', 'thus', ',', 'by', 'using', 'the', 'dom', 'to', 'display', 'our', 'ide', ',', 'we', 'can', 'leverage', 'these', 'investments', 'now', 'and', 'in', 'the', 'future', '.'], ['the', 'libraries', 'created', 'in', 'this', 'project', 'will', 'be', 'used', 'to', 'make', 'two', 'web-based', 'environments', '--', 'wescheme', 'and', 'code.pyret.org', '--', 'accessible', ',', 'and', 'those', 'environments', 'will', 'be', 'used', 'in', 'conducting', 'usability', 'testing', 'with', 'visually-impaired', 'users', '.']], 'tgt': [['no', 'summary', 'found']]}]\u001b[0m\n",
      "\u001b[33m[]\u001b[0m\n",
      "\u001b[33m[('../json_data/cnndm.valid.0.json', Namespace(dataset='', log_file='../logs/preprocess.log', lower=True, map_path='../data/', max_nsents=100, max_src_ntokens=200, min_nsents=3, min_src_ntokens=5, mode='format_to_bert', n_cpus=4, oracle_mode='greedy', raw_path='../json_data', save_path='../json_data', shard_size=2000), '../json_data/cnndm.valid.0.bert.pt')]\u001b[0m\n",
      "\u001b[33m[2020-04-16 18:22:13,910 INFO] loading vocabulary file ../json_data/vocab.txt\u001b[0m\n",
      "\u001b[33m[2020-04-16 18:22:13,937 INFO] Processing ../json_data/cnndm.valid.0.json\u001b[0m\n",
      "\u001b[33m[2020-04-16 18:22:13,942 INFO] Saving to ../json_data/cnndm.valid.0.bert.pt\u001b[0m\n",
      "\u001b[33m[('../json_data/cnndm.test.0.json', Namespace(dataset='', log_file='../logs/preprocess.log', lower=True, map_path='../data/', max_nsents=100, max_src_ntokens=200, min_nsents=3, min_src_ntokens=5, mode='format_to_bert', n_cpus=4, oracle_mode='greedy', raw_path='../json_data', save_path='../json_data', shard_size=2000), '../json_data/cnndm.test.0.bert.pt')]\u001b[0m\n",
      "\u001b[33m[2020-04-16 18:22:14,020 INFO] loading vocabulary file ../json_data/vocab.txt\u001b[0m\n",
      "\u001b[33m[2020-04-16 18:22:14,047 INFO] Processing ../json_data/cnndm.test.0.json\u001b[0m\n",
      "\u001b[33m[2020-04-16 18:22:14,052 INFO] Saving to ../json_data/cnndm.test.0.bert.pt\u001b[0m\n",
      "\u001b[33mlook at me: ../json_data/cnndm test\u001b[0m\n",
      "\u001b[33mfiles found :  ['../json_data/cnndm.test.0.bert.pt']\u001b[0m\n",
      "\u001b[33mgpu_rank 0\u001b[0m\n",
      "\u001b[33m------------inside train.py test_iter---------------- <models.data_loader.Dataloader object at 0x7f03dff347f0>\u001b[0m\n",
      "\u001b[33m---------------sent_scores-------------- tensor([[0.6188, 0.1393, 0.4561, 0.1400, 0.0958, 0.1075, 0.0228, 0.1870]])\u001b[0m\n",
      "\u001b[33m---------------mask--------------------- tensor([[1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.uint8)\u001b[0m\n",
      "\u001b[33m-----------------inside train.py pred------------------ ['brown university proposes a project to create a flexible , language agnostic software library that makes text and block programming tools accessible to visually-impaired users .<q>this project will produce a collection of javascript libraries that annotate source code with structural descriptions suitable for presentation by screen readers .<q>javascript was chosen because web-based environments address common technical and logistical challenges in schools .']\u001b[0m\n",
      "\u001b[33m-----------------inside train.py gold------------------ ['no summary found']\u001b[0m\n",
      "\u001b[33m<_io.TextIOWrapper name='../json_data/results/_step50000.candidate' mode='w' encoding='ANSI_X3.4-1968'>\u001b[0m\n",
      "\u001b[33m-----combined----- brown university proposes a project to create a flexible , language agnostic software library that makes text and block programming tools accessible to visually-impaired users .<q>this project will produce a collection of javascript libraries that annotate source code with structural descriptions suitable for presentation by screen readers .<q>javascript was chosen because web-based environments address common technical and logistical challenges in schools .\n",
      "\u001b[0m\n",
      "\u001b[33m---------------intermediate---------- brown university proposes a project to create a flexible , language agnostic software library that makes text and block programming tools accessible to visually-impaired users .<q>this project will produce a collection of javascript libraries that annotate source code with structural descriptions suitable for presentation by screen readers .<q>javascript was chosen because web-based environments address common technical and logistical challenges in schools .\n",
      "\u001b[0m\n",
      "\u001b[33m------------in pretty summary----------------- brown university proposes a project to create a flexible , language agnostic software library that makes text and block programming tools accessible to visually-impaired users .<q>this project will produce a collection of javascript libraries that annotate source code with structural descriptions suitable for presentation by screen readers .<q>javascript was chosen because web-based environments address common technical and logistical challenges in schools .\n",
      "\u001b[0m\n",
      "\u001b[33m------------------result------------- Brown university proposes a project to create a flexible , language agnostic software library that makes text and block programming tools accessible to visually-impaired users. This project will produce a collection of javascript libraries that annotate source code with structural descriptions suitable for presentation by screen readers. Javascript was chosen because web-based environments address common technical and logistical challenges in schools.\u001b[0m\n",
      "\u001b[33m169.254.255.130 - - [16/Apr/2020 18:22:14] \"#033[37mPOST /invocations HTTP/1.1#033[0m\" 200 -\u001b[0m\n",
      "Batch Transform complete\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import uuid\n",
    "transformer = model.transformer(2, 'ml.m5.xlarge')\n",
    "transformer.output_path = \"s3://nsfdata/summerized-abstract\"\n",
    "transformer.transform(top1_txt, content_type='text/plain')\n",
    "transformer.wait()\n",
    "print(\"Batch Transform complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YThSIa-ehnGX"
   },
   "source": [
    "## Output from Batch Transform\n",
    "\n",
    "Note: Ensure that the following package is installed on the local system : boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOehVNlDhnGX",
    "outputId": "1dda1784-b3a7-4a4b-9820-955c43c55fbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://nsfdata/summerized-abstract\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "print(transformer.output_path)\n",
    "bucketFolder = transformer.output_path.rsplit('/')[3]\n",
    "#print(s3bucket,s3prefix)\n",
    "s3_conn = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IelofWEDhnGZ",
    "outputId": "c80e27bc-c3be-4fa7-d20c-1cdfb0b48481"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file loaded from bucket\n"
     ]
    }
   ],
   "source": [
    "bucket_name=\"nsfdata\"\n",
    "with open('result.txt', 'wb') as f:\n",
    "    s3_conn.download_fileobj(bucket_name,bucketFolder+'/top1.txt.out', f)\n",
    "    print(\"Output file loaded from bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ShU-mX2fhnGb"
   },
   "outputs": [],
   "source": [
    "with open('./result.txt', 'rb') as file_stream:\n",
    "    output_text = file_stream.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-2Ydu5tPhnGc"
   },
   "source": [
    "#### Original Text Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lC20mx4MhnGd",
    "outputId": "3bdd6ae8-4deb-49ea-9b68-d04e5aa8062b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This study utilizes publicly available data from the National Science Foundation (NSF) Web Application Programming Interface (API). In this paper, various machine learning techniques are demonstrated to explore, analyze and recommend similar proposal abstracts to aid the NSF or Awardee with the Merit Review Process. These techniques extract textual context and group it with similar context. The goal of the analysis was to utilize a Doc2Vec unsupervised learning algorithms to embed NSF funding proposal abstracts text into vector space.  Once vectorized, the abstracts were grouped together using K-means clustering. These techniques together proved to be successful at grouping similar proposals together and could be used to find similar proposals to newly submitted NSF funding proposals. \r\n",
      "To perform text analysis, SAS® University Edition is used which supports SASPy, SAS® Studio and Python JupyterLab. Gensim Doc2vec is used to generate document vectors for proposal abstracts. Afterwards, document vectors were used to cluster similar abstracts using SAS® Studio KMeans Clustering Module. For visualization, the abstract embeddings were reduced to two dimensions using Principal Component Analysis (PCA) within SAS® Studio. This was then compared to a t-Distributed Stochastic Neighbor Embedding (t-SNE) dimensionality reduction technique as part of the Scikit-learn machine learning toolkit for Python.\r\n",
      "Conclusively, NSF proposal abstract text analysis can help an awardee read and improve their proposal model by identifying similar proposal abstracts from the last 24 years. It could also help NSF evaluators identify similar existing proposals that indirectly provides insights on whether a new proposal is going to be fruitful or not.\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UL2o6hDqhnGf"
   },
   "source": [
    "#### Similar abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Fnb5-1ShnGg",
    "outputId": "b86d3fe5-feed-4588-c3e7-33520c8ca5f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file loaded from bucket\n"
     ]
    }
   ],
   "source": [
    "bucket_name=\"nsfdata\"\n",
    "with open('top1.txt', 'wb') as f:\n",
    "    s3_conn.download_fileobj(bucket_name,'top1.txt', f)\n",
    "    print(\"Output file loaded from bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pyr6R3KXhnGi"
   },
   "outputs": [],
   "source": [
    "with open('./top1.txt', 'rb') as file_stream:\n",
    "    top1_text = file_stream.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "je59I3LQhnGj",
    "outputId": "882fc12a-fead-4bcf-c6cf-6130f0dd9a4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brown university proposes a project to create a flexible, language agnostic software library that makes text and block programming tools accessible to visually-impaired users.  screen-readers, the most common affordable tool for presenting text to those who can't read it directly, have difficulties presenting source code which is symbol-heavy and navigated best not as a list of words, but as an abstract syntax tree (ast). this project will produce a collection of javascript libraries that annotate source code with structural descriptions suitable for presentation by screen readers. \n",
      "\n",
      "javascript was chosen because web-based environments address common technical and logistical challenges in schools. they place fewer demands on it staff for installation and maintenance, and enable students to work outside of school without manually transferring files. web-based software is also easier to upgrade and evolve on the developer's end. being the lingua-franca for cloud-based uis, the browser's document object model (dom) is currently receiving significant attention when it comes to accessibility and thus, by using the dom to display our ide, we can leverage these investments now and in the future. the libraries created in this project will be used to make two web-based environments--wescheme and code.pyret.org--accessible, and those environments will be used in conducting usability testing with visually-impaired users.\n"
     ]
    }
   ],
   "source": [
    "print(top1_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DyNXd1R8hnGl"
   },
   "source": [
    "#### Summary of similar abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bLaDnrJjhnGm",
    "outputId": "563a5a21-492f-42a1-ec89-8de7dbbf2e6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown university proposes a project to create a flexible , language agnostic software library that makes text and block programming tools accessible to visually-impaired users. This project will produce a collection of javascript libraries that annotate source code with structural descriptions suitable for presentation by screen readers. Javascript was chosen because web-based environments address common technical and logistical challenges in schools.\n",
      "Execution time : 1.50seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NSFFinal.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
